{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f5a519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "986e7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374936ec",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d4540ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "HR = pd.read_csv('C:/Users/Nithin/Downloads/HR_ANN/HR_comma_sep.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b190d7",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f24da0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   satisfaction_level     14999 non-null  float64\n",
      " 1   last_evaluation        14999 non-null  float64\n",
      " 2   number_project         14999 non-null  int64  \n",
      " 3   average_montly_hours   14999 non-null  int64  \n",
      " 4   time_spend_company     14999 non-null  int64  \n",
      " 5   Work_accident          14999 non-null  int64  \n",
      " 6   left                   14999 non-null  int64  \n",
      " 7   promotion_last_5years  14999 non-null  int64  \n",
      " 8   department             14999 non-null  object \n",
      " 9   salary                 14999 non-null  object \n",
      "dtypes: float64(2), int64(6), object(2)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "HR.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43e40aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "satisfaction_level       0\n",
       "last_evaluation          0\n",
       "number_project           0\n",
       "average_montly_hours     0\n",
       "time_spend_company       0\n",
       "Work_accident            0\n",
       "left                     0\n",
       "promotion_last_5years    0\n",
       "department               0\n",
       "salary                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HR.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bce95064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Categorical Features\n",
    "numerical_features = ['satisfaction_level', 'last_evaluation', 'number_project',\n",
    "     'average_montly_hours', 'time_spend_company']\n",
    "\n",
    "categorical_features = ['Work_accident','promotion_last_5years', 'department', 'salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "055e8b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An utility function to create dummy variable\n",
    "def create_dummies( df, colname ):\n",
    "    col_dummies = pd.get_dummies(df[colname], prefix=colname)\n",
    "    col_dummies.drop(col_dummies.columns[0], axis=1, inplace=True)\n",
    "    df = pd.concat([df, col_dummies], axis=1)\n",
    "    df.drop( colname, axis = 1, inplace = True )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e47cde5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_feature in categorical_features:\n",
    "  HR = create_dummies( HR, c_feature )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb260a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "\n",
    "feature_columns = HR.columns.difference( ['left'] )\n",
    "feature_columns1 = feature_columns[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c41f7",
   "metadata": {},
   "source": [
    "### Train & Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10820f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(HR[feature_columns],HR['left'],test_size=0.3,random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e898ba",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44eb95ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SC = StandardScaler()\n",
    "\n",
    "SC.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86039743",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = SC.transform(train_x)\n",
    "test_x = SC.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f032f577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60582232\n",
      "Iteration 2, loss = 0.56701996\n",
      "Iteration 3, loss = 0.53749485\n",
      "Iteration 4, loss = 0.51480728\n",
      "Iteration 5, loss = 0.49678871\n",
      "Iteration 6, loss = 0.48225111\n",
      "Iteration 7, loss = 0.46882798\n",
      "Iteration 8, loss = 0.45646837\n",
      "Iteration 9, loss = 0.44519826\n",
      "Iteration 10, loss = 0.43616989\n",
      "Iteration 11, loss = 0.42856800\n",
      "Iteration 12, loss = 0.42187459\n",
      "Iteration 13, loss = 0.41548496\n",
      "Iteration 14, loss = 0.40924122\n",
      "Iteration 15, loss = 0.40302892\n",
      "Iteration 16, loss = 0.39671322\n",
      "Iteration 17, loss = 0.39004831\n",
      "Iteration 18, loss = 0.38315689\n",
      "Iteration 19, loss = 0.37624840\n",
      "Iteration 20, loss = 0.36933137\n",
      "Iteration 21, loss = 0.36299653\n",
      "Iteration 22, loss = 0.35757035\n",
      "Iteration 23, loss = 0.35266242\n",
      "Iteration 24, loss = 0.34827637\n",
      "Iteration 25, loss = 0.34418550\n",
      "Iteration 26, loss = 0.34040871\n",
      "Iteration 27, loss = 0.33677674\n",
      "Iteration 28, loss = 0.33346392\n",
      "Iteration 29, loss = 0.33036498\n",
      "Iteration 30, loss = 0.32751283\n",
      "Iteration 31, loss = 0.32482572\n",
      "Iteration 32, loss = 0.32245081\n",
      "Iteration 33, loss = 0.32004264\n",
      "Iteration 34, loss = 0.31788295\n",
      "Iteration 35, loss = 0.31573904\n",
      "Iteration 36, loss = 0.31356494\n",
      "Iteration 37, loss = 0.31151732\n",
      "Iteration 38, loss = 0.30955196\n",
      "Iteration 39, loss = 0.30764638\n",
      "Iteration 40, loss = 0.30560312\n",
      "Iteration 41, loss = 0.30343293\n",
      "Iteration 42, loss = 0.30115743\n",
      "Iteration 43, loss = 0.29862493\n",
      "Iteration 44, loss = 0.29617835\n",
      "Iteration 45, loss = 0.29366908\n",
      "Iteration 46, loss = 0.29136810\n",
      "Iteration 47, loss = 0.28907187\n",
      "Iteration 48, loss = 0.28701190\n",
      "Iteration 49, loss = 0.28501522\n",
      "Iteration 50, loss = 0.28303577\n",
      "Iteration 51, loss = 0.28118476\n",
      "Iteration 52, loss = 0.27934389\n",
      "Iteration 53, loss = 0.27747691\n",
      "Iteration 54, loss = 0.27561894\n",
      "Iteration 55, loss = 0.27389757\n",
      "Iteration 56, loss = 0.27220925\n",
      "Iteration 57, loss = 0.27062713\n",
      "Iteration 58, loss = 0.26901189\n",
      "Iteration 59, loss = 0.26758030\n",
      "Iteration 60, loss = 0.26594383\n",
      "Iteration 61, loss = 0.26448211\n",
      "Iteration 62, loss = 0.26314800\n",
      "Iteration 63, loss = 0.26167724\n",
      "Iteration 64, loss = 0.26026343\n",
      "Iteration 65, loss = 0.25888696\n",
      "Iteration 66, loss = 0.25757586\n",
      "Iteration 67, loss = 0.25629178\n",
      "Iteration 68, loss = 0.25515122\n",
      "Iteration 69, loss = 0.25401865\n",
      "Iteration 70, loss = 0.25299696\n",
      "Iteration 71, loss = 0.25183009\n",
      "Iteration 72, loss = 0.25086017\n",
      "Iteration 73, loss = 0.24973531\n",
      "Iteration 74, loss = 0.24871632\n",
      "Iteration 75, loss = 0.24773878\n",
      "Iteration 76, loss = 0.24667347\n",
      "Iteration 77, loss = 0.24569287\n",
      "Iteration 78, loss = 0.24481886\n",
      "Iteration 79, loss = 0.24402260\n",
      "Iteration 80, loss = 0.24318115\n",
      "Iteration 81, loss = 0.24250200\n",
      "Iteration 82, loss = 0.24156980\n",
      "Iteration 83, loss = 0.24085651\n",
      "Iteration 84, loss = 0.24023044\n",
      "Iteration 85, loss = 0.23953391\n",
      "Iteration 86, loss = 0.23882298\n",
      "Iteration 87, loss = 0.23812151\n",
      "Iteration 88, loss = 0.23763445\n",
      "Iteration 89, loss = 0.23702920\n",
      "Iteration 90, loss = 0.23654098\n",
      "Iteration 91, loss = 0.23591974\n",
      "Iteration 92, loss = 0.23548594\n",
      "Iteration 93, loss = 0.23497114\n",
      "Iteration 94, loss = 0.23448066\n",
      "Iteration 95, loss = 0.23394138\n",
      "Iteration 96, loss = 0.23362642\n",
      "Iteration 97, loss = 0.23328391\n",
      "Iteration 98, loss = 0.23287037\n",
      "Iteration 99, loss = 0.23234095\n",
      "Iteration 100, loss = 0.23205588\n",
      "Iteration 101, loss = 0.23162278\n",
      "Iteration 102, loss = 0.23150114\n",
      "Iteration 103, loss = 0.23110512\n",
      "Iteration 104, loss = 0.23083908\n",
      "Iteration 105, loss = 0.23057107\n",
      "Iteration 106, loss = 0.23034069\n",
      "Iteration 107, loss = 0.22989696\n",
      "Iteration 108, loss = 0.22969987\n",
      "Iteration 109, loss = 0.22930694\n",
      "Iteration 110, loss = 0.22909222\n",
      "Iteration 111, loss = 0.22889435\n",
      "Iteration 112, loss = 0.22867819\n",
      "Iteration 113, loss = 0.22859597\n",
      "Iteration 114, loss = 0.22834438\n",
      "Iteration 115, loss = 0.22808891\n",
      "Iteration 116, loss = 0.22791137\n",
      "Iteration 117, loss = 0.22774324\n",
      "Iteration 118, loss = 0.22759924\n",
      "Iteration 119, loss = 0.22723562\n",
      "Iteration 120, loss = 0.22701011\n",
      "Iteration 121, loss = 0.22694559\n",
      "Iteration 122, loss = 0.22676473\n",
      "Iteration 123, loss = 0.22668961\n",
      "Iteration 124, loss = 0.22650026\n",
      "Iteration 125, loss = 0.22637613\n",
      "Iteration 126, loss = 0.22609103\n",
      "Iteration 127, loss = 0.22593368\n",
      "Iteration 128, loss = 0.22593660\n",
      "Iteration 129, loss = 0.22575897\n",
      "Iteration 130, loss = 0.22550751\n",
      "Iteration 131, loss = 0.22545582\n",
      "Iteration 132, loss = 0.22531708\n",
      "Iteration 133, loss = 0.22534304\n",
      "Iteration 134, loss = 0.22500687\n",
      "Iteration 135, loss = 0.22502561\n",
      "Iteration 136, loss = 0.22466764\n",
      "Iteration 137, loss = 0.22470564\n",
      "Iteration 138, loss = 0.22454739\n",
      "Iteration 139, loss = 0.22453258\n",
      "Iteration 140, loss = 0.22426906\n",
      "Iteration 141, loss = 0.22416979\n",
      "Iteration 142, loss = 0.22408470\n",
      "Iteration 143, loss = 0.22401413\n",
      "Iteration 144, loss = 0.22383875\n",
      "Iteration 145, loss = 0.22377651\n",
      "Iteration 146, loss = 0.22375406\n",
      "Iteration 147, loss = 0.22348256\n",
      "Iteration 148, loss = 0.22352774\n",
      "Iteration 149, loss = 0.22330949\n",
      "Iteration 150, loss = 0.22331254\n",
      "Iteration 151, loss = 0.22330406\n",
      "Iteration 152, loss = 0.22315475\n",
      "Iteration 153, loss = 0.22313204\n",
      "Iteration 154, loss = 0.22302527\n",
      "Iteration 155, loss = 0.22278503\n",
      "Iteration 156, loss = 0.22286360\n",
      "Iteration 157, loss = 0.22292610\n",
      "Iteration 158, loss = 0.22275177\n",
      "Iteration 159, loss = 0.22259279\n",
      "Iteration 160, loss = 0.22252689\n",
      "Iteration 161, loss = 0.22240432\n",
      "Iteration 162, loss = 0.22246002\n",
      "Iteration 163, loss = 0.22239865\n",
      "Iteration 164, loss = 0.22243240\n",
      "Iteration 165, loss = 0.22231160\n",
      "Iteration 166, loss = 0.22220437\n",
      "Iteration 167, loss = 0.22213858\n",
      "Iteration 168, loss = 0.22218066\n",
      "Iteration 169, loss = 0.22217352\n",
      "Iteration 170, loss = 0.22195287\n",
      "Iteration 171, loss = 0.22202246\n",
      "Iteration 172, loss = 0.22187792\n",
      "Iteration 173, loss = 0.22186349\n",
      "Iteration 174, loss = 0.22194462\n",
      "Iteration 175, loss = 0.22207225\n",
      "Iteration 176, loss = 0.22162248\n",
      "Iteration 177, loss = 0.22166268\n",
      "Iteration 178, loss = 0.22152919\n",
      "Iteration 179, loss = 0.22171820\n",
      "Iteration 180, loss = 0.22167055\n",
      "Iteration 181, loss = 0.22158856\n",
      "Iteration 182, loss = 0.22140447\n",
      "Iteration 183, loss = 0.22153153\n",
      "Iteration 184, loss = 0.22141987\n",
      "Iteration 185, loss = 0.22144514\n",
      "Iteration 186, loss = 0.22133886\n",
      "Iteration 187, loss = 0.22126773\n",
      "Iteration 188, loss = 0.22131776\n",
      "Iteration 189, loss = 0.22131638\n",
      "Iteration 190, loss = 0.22124578\n",
      "Iteration 191, loss = 0.22131948\n",
      "Iteration 192, loss = 0.22107975\n",
      "Iteration 193, loss = 0.22123703\n",
      "Iteration 194, loss = 0.22125180\n",
      "Iteration 195, loss = 0.22121369\n",
      "Iteration 196, loss = 0.22119469\n",
      "Iteration 197, loss = 0.22113684\n",
      "Iteration 198, loss = 0.22109017\n",
      "Iteration 199, loss = 0.22097699\n",
      "Iteration 200, loss = 0.22089276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nithin\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=3, verbose=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP = MLPClassifier(hidden_layer_sizes=(3),verbose=True)\n",
    "\n",
    "MLP.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f007e",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82d41efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = MLP.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebd1a363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3290,  178],\n",
       "       [ 152,  880]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y,predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e20f075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      3468\n",
      "           1       0.83      0.85      0.84      1032\n",
      "\n",
      "    accuracy                           0.93      4500\n",
      "   macro avg       0.89      0.90      0.90      4500\n",
      "weighted avg       0.93      0.93      0.93      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y,predictors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f055ed6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(MLP.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a27cdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(MLP.coefs_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "207bbd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(MLP.intercepts_[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
